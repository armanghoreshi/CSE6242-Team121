{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76524864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import fredapi as fa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c00990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fredapi import Fred\n",
    "fred_key = \"d34fe23431a301d61901411501067ecb\"\n",
    "fred = Fred(fred_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95983c4d",
   "metadata": {},
   "source": [
    "Fetch unemployment data for counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2174a1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fred.search(\"unemployment rate\")\n",
    "#unemployment.columns\n",
    "#display(unemployment)\n",
    "#unemployment.dtypes\n",
    "df['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff1db78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df['title'].str.contains('Unemployment Rate in')]\n",
    "df3 = df2[df2['frequency'].str.startswith('Monthly')]\n",
    "df4 = df3[df3['title'].str.contains('County')]\n",
    "#len(df3)\n",
    "#display(df4)\n",
    "#df3['popularity']\n",
    "#df4.to_csv('raw_data_umemploy.csv', index=False)\n",
    "unemployment_by_county = {}\n",
    "#display(df4.title)\n",
    "for series_id in df4.index:\n",
    "    #display(series_id)\n",
    "    unemployment_by_county[df4.title[series_id]] = fred.get_series(series_id)\n",
    "    unemployment_by_county[df4.title[series_id]] = unemployment_by_county[df4.title[series_id]].dropna()\n",
    "    #display(unemployment_by_county[series_id])\n",
    "    #print(unemployment_by_county[series_id].index)\n",
    "    filt = unemployment_by_county[df4.title[series_id]].index > \"2016-12-01\"\n",
    "    unemployment_by_county[df4.title[series_id]] = unemployment_by_county[df4.title[series_id]][filt]\n",
    "    filt2 = unemployment_by_county[df4.title[series_id]].index < \"2024-01-01\"\n",
    "    unemployment_by_county[df4.title[series_id]] = unemployment_by_county[df4.title[series_id]][filt2]\n",
    "    #print(unemployment_by_county[series_id].index)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddf4feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment_by_county = pd.DataFrame(unemployment_by_county).T\n",
    "display(unemployment_by_county)\n",
    "unemployment_by_county.T.to_csv('./Input_Data/unemployment_by_county.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2f7684",
   "metadata": {},
   "source": [
    "Fetch Median Household income data for counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39010d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mh = fred.search(\"Estimate of Median Household Income for\")\n",
    "#unemployment.columns\n",
    "display(df_mh)\n",
    "#unemployment.dtypes\n",
    "#df_mh['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db1e758",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mh2 = df_mh[df_mh['title'].str.contains('Estimate of Median Household Income for')]\n",
    "#df_mh3 = df_mh2[df_mh2['frequency'].str.startswith('Monthly')]\n",
    "df_mh4 = df_mh2[df_mh2['title'].str.contains('County')]\n",
    "#len(df3)\n",
    "#display(df_mh4.title[1])\n",
    "#df3['popularity']\n",
    "#df4.to_csv('raw_data_umemploy.csv', index=False)\n",
    "median_income_by_county = {}\n",
    "count = 0 \n",
    "#display(df4.title)\n",
    "for series_id in df_mh4.index:\n",
    "    #display(series_id)\n",
    "    median_income_by_county[df_mh4.title[series_id]] = fred.get_series(series_id)\n",
    "    median_income_by_county[df_mh4.title[series_id]] = median_income_by_county[df_mh4.title[series_id]].dropna()\n",
    "     \n",
    "    #display(unemployment_by_county[series_id])\n",
    "    #print(median_income_by_county[df_mh4.title[series_id]])\n",
    "    filt = median_income_by_county[df_mh4.title[series_id]].index > \"2016-01-01\"\n",
    "    median_income_by_county[df_mh4.title[series_id]] = median_income_by_county[df_mh4.title[series_id]][filt]\n",
    "    count += 1\n",
    "    print(count)\n",
    "    time.sleep(0.5)\n",
    "    #filt2 = median_income_by_county[df_mh4.title[series_id]].index < \"2024-01-01\"\n",
    "    #median_income_by_county[df_mh4.title[series_id]] = median_income_by_county[df_mh4.title[series_id]][filt2]\n",
    "    #print(median_income_by_county[df_mh4.title[series_id]].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b88f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_income_by_county = pd.DataFrame(median_income_by_county).T\n",
    "display(median_income_by_county)\n",
    "median_income_by_county.T.to_csv('./Input_Data/median_income_by_county.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1484a58d",
   "metadata": {},
   "source": [
    "homeownership ratio for each county data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cdf146",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ho = fred.search(\" Homeownership Rate (5-year estimate) for\")\n",
    "#unemployment.columns\n",
    "display(df_ho)\n",
    "#unemployment.dtypes\n",
    "#df_mh['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8536f8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ho2 = df_ho[df_ho['title'].str.contains('County')]\n",
    "#len(df3)\n",
    "#display(df_mh4.title[1])\n",
    "#df3['popularity']\n",
    "#df4.to_csv('raw_data_umemploy.csv', index=False)\n",
    "homeowner_by_county = {}\n",
    "count = 0 \n",
    "#display(df4.title)\n",
    "for series_id in df_ho2.index:\n",
    "    #display(series_id)\n",
    "    homeowner_by_county[df_ho2.title[series_id]] = fred.get_series(series_id)\n",
    "    homeowner_by_county[df_ho2.title[series_id]] = homeowner_by_county[df_ho2.title[series_id]].dropna()\n",
    "    filt = homeowner_by_county[df_ho2.title[series_id]].index > \"2016-01-01\"\n",
    "    homeowner_by_county[df_ho2.title[series_id]] = homeowner_by_county[df_ho2.title[series_id]][filt]\n",
    "    count += 1\n",
    "    print(count)\n",
    "    time.sleep(0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04815040",
   "metadata": {},
   "outputs": [],
   "source": [
    "homeowner_by_county = pd.DataFrame(homeowner_by_county).T\n",
    "display(homeowner_by_county)\n",
    "homeowner_by_county.T.to_csv('./Input_Data/homeowner_by_county.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f783bd5d",
   "metadata": {},
   "source": [
    "download income inequality ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c521a2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ii = fred.search(\"Income Inequality in\")\n",
    "#unemployment.columns\n",
    "display(df_ii)\n",
    "#unemployment.dtypes\n",
    "#df_mh['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6016f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ii2 = df_ii[df_ii['title'].str.contains('County')]\n",
    "#len(df3)\n",
    "#display(df_mh4.title[1])\n",
    "#df3['popularity']\n",
    "#df4.to_csv('raw_data_umemploy.csv', index=False)\n",
    "income_inequality_by_county = {}\n",
    "count = 0 \n",
    "#display(df4.title)\n",
    "for series_id in df_ii2.index:\n",
    "    #display(series_id)\n",
    "    income_inequality_by_county[df_ii2.title[series_id]] = fred.get_series(series_id)\n",
    "    income_inequality_by_county[df_ii2.title[series_id]] = income_inequality_by_county[df_ii2.title[series_id]].dropna()\n",
    "    filt = income_inequality_by_county[df_ii2.title[series_id]].index > \"2016-01-01\"\n",
    "    income_inequality_by_county[df_ii2.title[series_id]] = income_inequality_by_county[df_ii2.title[series_id]][filt]\n",
    "    count += 1\n",
    "    print(count)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862ee060",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_inequality_by_county = pd.DataFrame(income_inequality_by_county).T\n",
    "display(income_inequality_by_county)\n",
    "income_inequality_by_county.T.to_csv('./Input_Data/income_inequality_by_county.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c83896",
   "metadata": {},
   "source": [
    "fetch Mean Commuting Time for Workers (5-year estimate) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22737a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mc = fred.search(\"Mean Commuting Time for Workers (5-year estimate) in\")\n",
    "#unemployment.columns\n",
    "display(df_mc)\n",
    "#unemployment.dtypes\n",
    "#df_mh['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b585d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mc2 = df_mc[df_mc['title'].str.contains('County')]\n",
    "#len(df3)\n",
    "#display(df_mh4.title[1])\n",
    "#df3['popularity']\n",
    "#df4.to_csv('raw_data_umemploy.csv', index=False)\n",
    "meancommute_by_county = {}\n",
    "count = 0 \n",
    "#display(df4.title)\n",
    "for series_id in df_mc2.index:\n",
    "    #display(series_id)\n",
    "    meancommute_by_county[df_mc2.title[series_id]] = fred.get_series(series_id)\n",
    "    meancommute_by_county[df_mc2.title[series_id]] = meancommute_by_county[df_mc2.title[series_id]].dropna()\n",
    "    filt = meancommute_by_county[df_mc2.title[series_id]].index > \"2016-01-01\"\n",
    "    meancommute_by_county[df_mc2.title[series_id]] = meancommute_by_county[df_mc2.title[series_id]][filt]\n",
    "    count += 1\n",
    "    print(count)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cefc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "meancommute_by_county = pd.DataFrame(meancommute_by_county).T\n",
    "display(meancommute_by_county)\n",
    "meancommute_by_county.T.to_csv('./Input_Data/meancommute_by_county.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eabc68",
   "metadata": {},
   "source": [
    "fetch percent of population below poverty level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9e5171",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pp = fred.search(\"Percent of Population Below the Poverty Level (5-year estimate) in\")\n",
    "#unemployment.columns\n",
    "display(df_pp)\n",
    "#unemployment.dtypes\n",
    "#df_mh['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcf1669",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pp2 = df_pp[df_pp['title'].str.contains('County')]\n",
    "#len(df3)\n",
    "#display(df_mh4.title[1])\n",
    "#df3['popularity']\n",
    "#df4.to_csv('raw_data_umemploy.csv', index=False)\n",
    "poppoverty_by_county = {}\n",
    "count = 0 \n",
    "#display(df4.title)\n",
    "for series_id in df_pp2.index:\n",
    "    #display(series_id)\n",
    "    poppoverty_by_county[df_pp2.title[series_id]] = fred.get_series(series_id)\n",
    "    poppoverty_by_county[df_pp2.title[series_id]] = poppoverty_by_county[df_pp2.title[series_id]].dropna()\n",
    "    filt = poppoverty_by_county[df_pp2.title[series_id]].index > \"2016-01-01\"\n",
    "    poppoverty_by_county[df_pp2.title[series_id]] = poppoverty_by_county[df_pp2.title[series_id]][filt]\n",
    "    count += 1\n",
    "    print(count)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a48153",
   "metadata": {},
   "outputs": [],
   "source": [
    "poppoverty_by_county = pd.DataFrame(poppoverty_by_county).T\n",
    "display(poppoverty_by_county)\n",
    "poppoverty_by_county.T.to_csv('./Input_Data/poppoverty_by_county.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cba3a8a",
   "metadata": {},
   "source": [
    "fetch resident population for county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c5aa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rp = fred.search(\"Resident Population in\")\n",
    "#unemployment.columns\n",
    "display(df_rp)\n",
    "#unemployment.dtypes\n",
    "#df_mh['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051b736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rp2 = df_rp[df_rp['title'].str.contains('County')]\n",
    "#len(df3)\n",
    "#display(df_mh4.title[1])\n",
    "#df3['popularity']\n",
    "#df4.to_csv('raw_data_umemploy.csv', index=False)\n",
    "respop_by_county = {}\n",
    "count = 0 \n",
    "#display(df4.title)\n",
    "for series_id in df_rp2.index:\n",
    "    #display(series_id)\n",
    "    respop_by_county[df_rp2.title[series_id]] = fred.get_series(series_id)\n",
    "    respop_by_county[df_rp2.title[series_id]] = respop_by_county[df_rp2.title[series_id]].dropna()\n",
    "    filt = respop_by_county[df_rp2.title[series_id]].index > \"2016-01-01\"\n",
    "    respop_by_county[df_rp2.title[series_id]] = respop_by_county[df_rp2.title[series_id]][filt]\n",
    "    count += 1\n",
    "    print(count)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a184a015",
   "metadata": {},
   "outputs": [],
   "source": [
    "respop_by_county = pd.DataFrame(respop_by_county).T\n",
    "display(respop_by_county)\n",
    "respop_by_county.T.to_csv('./Input_Data/respop_by_county.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd9bb3e",
   "metadata": {},
   "source": [
    "Fetch home price index for county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3511e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pi = fred.search(\"All-Transactions House Price Index for\")\n",
    "#unemployment.columns\n",
    "display(df_pi)\n",
    "#unemployment.dtypes\n",
    "#df_mh['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa17b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pi2 = df_pi[df_pi['title'].str.contains('County')]\n",
    "#len(df3)\n",
    "#display(df_mh4.title[1])\n",
    "#df3['popularity']\n",
    "#df4.to_csv('raw_data_umemploy.csv', index=False)\n",
    "priceindex_by_county = {}\n",
    "count = 0 \n",
    "#display(df4.title)\n",
    "for series_id in df_pi2.index:\n",
    "    #display(series_id)\n",
    "    priceindex_by_county[df_pi2.title[series_id]] = fred.get_series(series_id)\n",
    "    priceindex_by_county[df_pi2.title[series_id]] = priceindex_by_county[df_pi2.title[series_id]].dropna()\n",
    "    filt = priceindex_by_county[df_pi2.title[series_id]].index > \"2016-01-01\"\n",
    "    priceindex_by_county[df_pi2.title[series_id]] = priceindex_by_county[df_pi2.title[series_id]][filt]\n",
    "    filt2 = priceindex_by_county[df_pi2.title[series_id]].index < \"2024-01-01\"\n",
    "    priceindex_by_county[df_pi2.title[series_id]] = priceindex_by_county[df_pi2.title[series_id]][filt2]\n",
    "    count += 1\n",
    "    print(count)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12eae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "priceindex_by_county = pd.DataFrame(priceindex_by_county).T\n",
    "display(priceindex_by_county)\n",
    "priceindex_by_county.T.to_csv('./Input_Data/priceindex_by_county.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9465b1",
   "metadata": {},
   "source": [
    "Fetch GDP data for county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c3c38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gdp = fred.search(\"Gross Domestic Product: All Industries in\")\n",
    "#unemployment.columns\n",
    "display(df_gdp)\n",
    "#unemployment.dtypes\n",
    "#df_mh['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e258658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gdp2 = df_gdp[df_gdp['title'].str.contains('County')]\n",
    "df_gdp3 = df_gdp2[df_gdp2['title'].str.startswith('Gross Domestic Product: All Industries in')]\n",
    "#len(df3)\n",
    "#display(df_mh4.title[1])\n",
    "#df3['popularity']\n",
    "#df4.to_csv('raw_data_umemploy.csv', index=False)\n",
    "gdp_by_county = {}\n",
    "count = 0 \n",
    "#display(df4.title)\n",
    "for series_id in df_gdp3.index:\n",
    "    #display(series_id)\n",
    "    gdp_by_county[df_gdp3.title[series_id]] = fred.get_series(series_id)\n",
    "    gdp_by_county[df_gdp3.title[series_id]] = gdp_by_county[df_gdp3.title[series_id]].dropna()\n",
    "    filt = gdp_by_county[df_gdp3.title[series_id]].index > \"2016-01-01\"\n",
    "    gdp_by_county[df_gdp3.title[series_id]] = gdp_by_county[df_gdp3.title[series_id]][filt]\n",
    "    filt2 = gdp_by_county[df_gdp3.title[series_id]].index < \"2024-01-01\"\n",
    "    gdp_by_county[df_gdp3.title[series_id]] = gdp_by_county[df_gdp3.title[series_id]][filt2]\n",
    "    count += 1\n",
    "    print(count)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fb6716",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_by_county = pd.DataFrame(gdp_by_county).T\n",
    "display(gdp_by_county)\n",
    "gdp_by_county.T.to_csv('./Input_Data/gdp_by_county.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a400b65",
   "metadata": {},
   "source": [
    "Fetch avg home price per county "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168dc401",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ap = fred.search(\"Housing Inventory: Average Listing Price in\")\n",
    "#unemployment.columns\n",
    "display(df_ap)\n",
    "#unemployment.dtypes\n",
    "#df_mh['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c8a687",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ap2 = df_ap[df_ap['title'].str.contains('County')]\n",
    "df_ap3 = df_ap2[~df_ap2['title'].str.contains('Month')]\n",
    "df_ap4 = df_ap3[~df_ap3['title'].str.contains('Year')]\n",
    "#len(df3)\n",
    "#display(df_mh4.title[1])\n",
    "#df3['popularity']\n",
    "#df4.to_csv('raw_data_umemploy.csv', index=False)\n",
    "avgprice_by_county = {}\n",
    "count = 0 \n",
    "#display(df4.title)\n",
    "for series_id in df_ap4.index:\n",
    "    #display(series_id)\n",
    "    avgprice_by_county[df_ap4.title[series_id]] = fred.get_series(series_id)\n",
    "    avgprice_by_county[df_ap4.title[series_id]] = avgprice_by_county[df_ap4.title[series_id]].dropna()\n",
    "    filt = avgprice_by_county[df_ap4.title[series_id]].index > \"2016-01-01\"\n",
    "    avgprice_by_county[df_ap4.title[series_id]] = avgprice_by_county[df_ap4.title[series_id]][filt]\n",
    "    filt2 = avgprice_by_county[df_ap4.title[series_id]].index < \"2024-01-01\"\n",
    "    avgprice_by_county[df_ap4.title[series_id]] = avgprice_by_county[df_ap4.title[series_id]][filt2]\n",
    "    count += 1\n",
    "    print(count)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee33ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgprice_by_county = pd.DataFrame(avgprice_by_county).T\n",
    "display(avgprice_by_county)\n",
    "avgprice_by_county.T.to_csv('./Input_Data/avgprice_by_county.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753633d7",
   "metadata": {},
   "source": [
    "Fetch active listing count per county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b56063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_al = fred.search(\"Housing Inventory: Active Listing Count in\")\n",
    "#unemployment.columns\n",
    "display(df_al)\n",
    "#unemployment.dtypes\n",
    "#df_mh['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929b16f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_al2 = df_al[df_al['title'].str.contains('County')]\n",
    "df_al3 = df_al2[~df_al2['title'].str.contains('Month')]\n",
    "df_al4 = df_al3[~df_al3['title'].str.contains('Year')]\n",
    "df_al5 = df_al4[~df_al4['title'].str.contains('Reduced')]\n",
    "df_al6 = df_al5[~df_al5['title'].str.contains('Increased')]\n",
    "df_al7 = df_al6[df_al6['title'].str.startswith('Housing Inventory: Active Listing Count in')]\n",
    "#len(df3)\n",
    "#display(df_mh4.title[1])\n",
    "#df3['popularity']\n",
    "#df4.to_csv('raw_data_umemploy.csv', index=False)\n",
    "actlist_by_county = {}\n",
    "count = 0 \n",
    "#display(df4.title)\n",
    "for series_id in df_al7.index:\n",
    "    #display(series_id)\n",
    "    actlist_by_county[df_al7.title[series_id]] = fred.get_series(series_id)\n",
    "    actlist_by_county[df_al7.title[series_id]] = actlist_by_county[df_al7.title[series_id]].dropna()\n",
    "    filt = actlist_by_county[df_al7.title[series_id]].index > \"2016-01-01\"\n",
    "    actlist_by_county[df_al7.title[series_id]] = actlist_by_county[df_al7.title[series_id]][filt]\n",
    "    filt2 = actlist_by_county[df_al7.title[series_id]].index < \"2024-01-01\"\n",
    "    actlist_by_county[df_al7.title[series_id]] = actlist_by_county[df_al7.title[series_id]][filt2]\n",
    "    count += 1\n",
    "    print(count)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e931bb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "actlist_by_county = pd.DataFrame(actlist_by_county).T\n",
    "display(actlist_by_county)\n",
    "actlist_by_county.T.to_csv('./Input_Data/actlist_by_county.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9b5432",
   "metadata": {},
   "source": [
    "Fetch new listing count for county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f91291",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nl = fred.search(\"Housing Inventory: New Listing Count in\")\n",
    "#unemployment.columns\n",
    "display(df_nl)\n",
    "#unemployment.dtypes\n",
    "#df_mh['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08671b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nl2 = df_nl[df_nl['title'].str.contains('County')]\n",
    "df_nl3 = df_nl2[~df_nl2['title'].str.contains('Month')]\n",
    "df_nl4 = df_nl3[~df_nl3['title'].str.contains('Year')]\n",
    "df_nl5 = df_nl4[~df_nl4['title'].str.contains('Reduced')]\n",
    "df_nl6 = df_nl5[~df_nl5['title'].str.contains('Increased')]\n",
    "df_nl7 = df_nl6[df_nl6['title'].str.startswith('Housing Inventory: New Listing Count in')]\n",
    "#len(df3)\n",
    "#display(df_mh4.title[1])\n",
    "#df3['popularity']\n",
    "#df4.to_csv('raw_data_umemploy.csv', index=False)\n",
    "newlist_by_county = {}\n",
    "count = 0 \n",
    "#display(df4.title)\n",
    "for series_id in df_nl7.index:\n",
    "    #display(series_id)\n",
    "    newlist_by_county[df_nl7.title[series_id]] = fred.get_series(series_id)\n",
    "    newlist_by_county[df_nl7.title[series_id]] = newlist_by_county[df_nl7.title[series_id]].dropna()\n",
    "    filt = newlist_by_county[df_nl7.title[series_id]].index > \"2016-01-01\"\n",
    "    newlist_by_county[df_nl7.title[series_id]] = newlist_by_county[df_nl7.title[series_id]][filt]\n",
    "    filt2 = newlist_by_county[df_nl7.title[series_id]].index < \"2024-01-01\"\n",
    "    newlist_by_county[df_nl7.title[series_id]] = newlist_by_county[df_nl7.title[series_id]][filt2]\n",
    "    count += 1\n",
    "    print(count)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c7b0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "newlist_by_county = pd.DataFrame(newlist_by_county).T\n",
    "display(newlist_by_county)\n",
    "newlist_by_county.T.to_csv('./Input_Data/newlist_by_county.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc001ce8",
   "metadata": {},
   "source": [
    "fetch for total listing count for county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac172da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tl = fred.search(\"Housing Inventory: Total Listing Count in\")\n",
    "#unemployment.columns\n",
    "display(df_tl)\n",
    "#unemployment.dtypes\n",
    "#df_mh['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e772fc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tl2 = df_tl[df_tl['title'].str.contains('County')]\n",
    "df_tl3 = df_tl2[~df_tl2['title'].str.contains('Month')]\n",
    "df_tl4 = df_tl3[~df_tl3['title'].str.contains('Year')]\n",
    "df_tl5 = df_tl4[df_tl4['title'].str.startswith('Housing Inventory: Total Listing Count in')]\n",
    "#len(df3)\n",
    "#display(df_mh4.title[1])\n",
    "#df3['popularity']\n",
    "#df4.to_csv('raw_data_umemploy.csv', index=False)\n",
    "totallist_by_county = {}\n",
    "count = 0 \n",
    "#display(df4.title)\n",
    "for series_id in df_tl5.index:\n",
    "    #display(series_id)\n",
    "    totallist_by_county[df_tl5.title[series_id]] = fred.get_series(series_id)\n",
    "    totallist_by_county[df_tl5.title[series_id]] = totallist_by_county[df_tl5.title[series_id]].dropna()\n",
    "    filt = totallist_by_county[df_tl5.title[series_id]].index > \"2016-01-01\"\n",
    "    totallist_by_county[df_tl5.title[series_id]] = totallist_by_county[df_tl5.title[series_id]][filt]\n",
    "    filt2 = totallist_by_county[df_tl5.title[series_id]].index < \"2024-01-01\"\n",
    "    totallist_by_county[df_tl5.title[series_id]] = totallist_by_county[df_tl5.title[series_id]][filt2]\n",
    "    count += 1\n",
    "    print(count)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5499b9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "totallist_by_county = pd.DataFrame(totallist_by_county).T\n",
    "display(totallist_by_county)\n",
    "totallist_by_county.T.to_csv('./Input_Data/totallist_by_county.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791901e1",
   "metadata": {},
   "source": [
    "Fetch Pending Listing count for county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0be6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pl = fred.search(\"Housing Inventory: Pending Listing Count in\")\n",
    "#unemployment.columns\n",
    "display(df_pl)\n",
    "#unemployment.dtypes\n",
    "#df_mh['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ebf280",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pl2 = df_pl[df_pl['title'].str.contains('County')]\n",
    "df_pl3 = df_pl2[~df_pl2['title'].str.contains('Month')]\n",
    "df_pl4 = df_pl3[~df_pl3['title'].str.contains('Year')]\n",
    "df_pl5 = df_pl4[df_pl4['title'].str.startswith('Housing Inventory: Pending Listing Count in')]\n",
    "#len(df3)\n",
    "#display(df_mh4.title[1])\n",
    "#df3['popularity']\n",
    "#df4.to_csv('raw_data_umemploy.csv', index=False)\n",
    "pendlist_by_county = {}\n",
    "count = 0 \n",
    "#display(df4.title)\n",
    "for series_id in df_pl5.index:\n",
    "    #display(series_id)\n",
    "    pendlist_by_county[df_pl5.title[series_id]] = fred.get_series(series_id)\n",
    "    pendlist_by_county[df_pl5.title[series_id]] = pendlist_by_county[df_pl5.title[series_id]].dropna()\n",
    "    filt = pendlist_by_county[df_pl5.title[series_id]].index > \"2016-01-01\"\n",
    "    pendlist_by_county[df_pl5.title[series_id]] = pendlist_by_county[df_pl5.title[series_id]][filt]\n",
    "    filt2 = pendlist_by_county[df_pl5.title[series_id]].index < \"2024-01-01\"\n",
    "    pendlist_by_county[df_pl5.title[series_id]] = pendlist_by_county[df_pl5.title[series_id]][filt2]\n",
    "    count += 1\n",
    "    print(count)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1134d25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pendlist_by_county = pd.DataFrame(pendlist_by_county).T\n",
    "display(pendlist_by_county)\n",
    "pendlist_by_county.T.to_csv('./Input_Data/pendlist_by_county.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95560ee",
   "metadata": {},
   "source": [
    "create county fips file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f8a6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file path for the original file\n",
    "file_path = './Input_Data/fips.txt'\n",
    "\n",
    "# Load the data from the file\n",
    "with open(file_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Process the lines to extract relevant data\n",
    "state_data = []\n",
    "county_data = []\n",
    "\n",
    "state_section = False\n",
    "county_section = False\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    # Check for state-level section\n",
    "    if line.startswith(\"state-level\"):\n",
    "        state_section = True\n",
    "        county_section = False\n",
    "        continue\n",
    "    # Check for county-level section\n",
    "    if line.startswith(\"county-level\"):\n",
    "        county_section = True\n",
    "        state_section = False\n",
    "        continue\n",
    "    \n",
    "    # Extract state-level FIPS code and state name\n",
    "    if state_section and line:\n",
    "        parts = line.split()\n",
    "        state_fips = parts[0]\n",
    "        state_name = \" \".join(parts[1:])\n",
    "        state_data.append((state_fips, state_name))\n",
    "    \n",
    "    # Extract county-level FIPS code and county name\n",
    "    if county_section and line:\n",
    "        parts = line.split()\n",
    "        county_fips = parts[0]\n",
    "        county_name = \" \".join(parts[1:])\n",
    "        # The state FIPS code is the first two characters of the county FIPS code\n",
    "        state_fips = county_fips[:2]\n",
    "        county_data.append((state_fips, county_fips, county_name))\n",
    "\n",
    "# Create dataframes\n",
    "state_df = pd.DataFrame(state_data, columns=[\"state_fips\", \"state_name\"])\n",
    "county_df = pd.DataFrame(county_data, columns=[\"state_fips\", \"county_fips\", \"county_name\"])\n",
    "\n",
    "# Merge state and county data on state FIPS code\n",
    "merged_df = pd.merge(county_df, state_df, on=\"state_fips\")\n",
    "\n",
    "# Rename columns\n",
    "merged_df.rename(columns={\n",
    "    \"state_fips\": \"StateCodeFIPS\",\n",
    "    \"state_name\": \"StateName\",\n",
    "    \"county_fips\": \"CountyFIPS\",\n",
    "    \"county_name\": \"CountyName\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Dictionary mapping state names to their abbreviations\n",
    "state_abbr_dict = {\n",
    "    'ALABAMA': 'AL', 'ALASKA': 'AK', 'ARIZONA': 'AZ', 'ARKANSAS': 'AR',\n",
    "    'CALIFORNIA': 'CA', 'COLORADO': 'CO', 'CONNECTICUT': 'CT', 'DELAWARE': 'DE',\n",
    "    'DISTRICT OF COLUMBIA': 'DC', 'FLORIDA': 'FL', 'GEORGIA': 'GA', 'HAWAII': 'HI',\n",
    "    'IDAHO': 'ID', 'ILLINOIS': 'IL', 'INDIANA': 'IN', 'IOWA': 'IA', 'KANSAS': 'KS',\n",
    "    'KENTUCKY': 'KY', 'LOUISIANA': 'LA', 'MAINE': 'ME', 'MARYLAND': 'MD',\n",
    "    'MASSACHUSETTS': 'MA', 'MICHIGAN': 'MI', 'MINNESOTA': 'MN', 'MISSISSIPPI': 'MS',\n",
    "    'MISSOURI': 'MO', 'MONTANA': 'MT', 'NEBRASKA': 'NE', 'NEVADA': 'NV',\n",
    "    'NEW HAMPSHIRE': 'NH', 'NEW JERSEY': 'NJ', 'NEW MEXICO': 'NM', 'NEW YORK': 'NY',\n",
    "    'NORTH CAROLINA': 'NC', 'NORTH DAKOTA': 'ND', 'OHIO': 'OH', 'OKLAHOMA': 'OK',\n",
    "    'OREGON': 'OR', 'PENNSYLVANIA': 'PA', 'RHODE ISLAND': 'RI', 'SOUTH CAROLINA': 'SC',\n",
    "    'SOUTH DAKOTA': 'SD', 'TENNESSEE': 'TN', 'TEXAS': 'TX', 'UTAH': 'UT',\n",
    "    'VERMONT': 'VT', 'VIRGINIA': 'VA', 'WASHINGTON': 'WA', 'WEST VIRGINIA': 'WV',\n",
    "    'WISCONSIN': 'WI', 'WYOMING': 'WY'\n",
    "}\n",
    "\n",
    "# Map the 'StateName' column to its abbreviation\n",
    "merged_df['StateName'] = merged_df['StateName'].map(state_abbr_dict)\n",
    "\n",
    "# Remove the word \"County\" from the values in the 'CountyName' column\n",
    "merged_df['CountyName'] = merged_df['CountyName'].str.replace(\" County\", \"\", regex=False)\n",
    "\n",
    "# Save the final DataFrame to a new CSV file\n",
    "output_path_final = \"./Input_Data/countyname.csv\"\n",
    "merged_df.to_csv(output_path_final, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70c8105",
   "metadata": {},
   "source": [
    "preprocess unemployment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7647041d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the unemployment dataset\n",
    "file_path = './Input_Data/unemployment_by_county.csv'\n",
    "unemployment_data = pd.read_csv(file_path)\n",
    "\n",
    "# Rotate columns and rows\n",
    "unemployment_data_rotated = unemployment_data.set_index('Unnamed: 0').transpose()\n",
    "\n",
    "# Extract County and State Names\n",
    "county_state_data = unemployment_data_rotated.index.str.extract(r'Unemployment Rate in (.+?), (\\w{2})')\n",
    "county_state_data.columns = ['County', 'State']\n",
    "\n",
    "# Add extracted columns to the rotated table\n",
    "unemployment_data_rotated_with_location = unemployment_data_rotated.copy()\n",
    "unemployment_data_rotated_with_location['County'] = county_state_data['County'].values\n",
    "unemployment_data_rotated_with_location['State'] = county_state_data['State'].values\n",
    "\n",
    "# Merge “Unemployment Rate” columns and name as \"UnemploymentRate\"\n",
    "unemployment_data_long = unemployment_data_rotated_with_location.reset_index().melt(\n",
    "    id_vars=['County', 'State'], \n",
    "    var_name='Date', \n",
    "    value_name='UnemploymentRate'\n",
    ")\n",
    "\n",
    "# Revert merged data back to the original wide format\n",
    "unemployment_data_wide = unemployment_data_long.pivot(index=['County', 'State'], columns='Date', values='UnemploymentRate').reset_index()\n",
    "\n",
    "# Rename columns and add \"RegionType\", \"State\", \"StateCodeFIPS\", \"county_fips\"\n",
    "unemployment_data_wide = unemployment_data_wide.rename(columns={'County': 'RegionName', 'State': 'StateName'})\n",
    "unemployment_data_wide['RegionType'] = 'county'\n",
    "unemployment_data_wide['State'] = unemployment_data_wide['StateName']\n",
    "\n",
    "# Adding State FIPS codes (sample, complete dictionary required)\n",
    "state_fips_codes = {\n",
    "    'AL': '01', 'AK': '02', 'AZ': '04', 'AR': '05', 'CA': '06', 'CO': '08', 'CT': '09', 'DE': '10',\n",
    "    'FL': '12', 'GA': '13', 'HI': '15', 'ID': '16', 'IL': '17', 'IN': '18', 'IA': '19', 'KS': '20',\n",
    "    'KY': '21', 'LA': '22', 'ME': '23', 'MD': '24', 'MA': '25', 'MI': '26', 'MN': '27', 'MS': '28',\n",
    "    'MO': '29', 'MT': '30', 'NE': '31', 'NV': '32', 'NH': '33', 'NJ': '34', 'NM': '35', 'NY': '36',\n",
    "    'NC': '37', 'ND': '38', 'OH': '39', 'OK': '40', 'OR': '41', 'PA': '42', 'RI': '44', 'SC': '45',\n",
    "    'SD': '46', 'TN': '47', 'TX': '48', 'UT': '49', 'VT': '50', 'VA': '51', 'WA': '53', 'WV': '54',\n",
    "    'WI': '55', 'WY': '56'\n",
    "}\n",
    "unemployment_data_wide['StateCodeFIPS'] = unemployment_data_wide['StateName'].map(state_fips_codes)\n",
    "unemployment_data_wide['county_fips'] = None  # Placeholder for county FIPS\n",
    "\n",
    "# Save final data to CSV\n",
    "output_file_path = './Input_Data/filled_unemployment_data_with_fips.csv'\n",
    "unemployment_data_wide.to_csv(output_file_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6173a440",
   "metadata": {},
   "source": [
    "add county FIPS code from  county fips code file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1b3b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load the original CSV file\n",
    "file_path = './Input_Data/filled_unemployment_data_with_fips.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Reorder columns as requested\n",
    "ordered_columns = [\"index\", \"RegionName\", \"RegionType\", \"StateName\", \"State\", \"StateCodeFIPS\", \"county_fips\"]\n",
    "remaining_columns = [col for col in data.columns if col not in ordered_columns]\n",
    "final_columns = ordered_columns + remaining_columns\n",
    "data = data[final_columns]\n",
    "\n",
    "# Create new \"CountyName\" column by modifying \"RegionName\" values\n",
    "data['CountyName'] = data['RegionName'].str.replace(\" County\", \"\", regex=True)\n",
    "\n",
    "# Reorder to place \"CountyName\" right after \"RegionName\"\n",
    "columns_reordered = data.columns.tolist()\n",
    "columns_reordered.insert(columns_reordered.index(\"RegionName\") + 1, columns_reordered.pop(columns_reordered.index(\"CountyName\")))\n",
    "data = data[columns_reordered]\n",
    "\n",
    "# Remove \"/City\" from the \"CountyName\" values\n",
    "data['CountyName'] = data['CountyName'].str.replace(\"/City\", \"\", regex=True)\n",
    "\n",
    "# Load the countyname.csv file to get FIPS codes\n",
    "county_fips_path = './Input_Data/countyname.csv'\n",
    "county_fips_data = pd.read_csv(county_fips_path)\n",
    "\n",
    "# Merge the two dataframes on 'CountyName' and 'StateName' to fill 'county_fips' from 'CountyFIPS'\n",
    "data = data.merge(county_fips_data[['CountyFIPS', 'CountyName', 'StateName']], on=['CountyName', 'StateName'], how='left')\n",
    "\n",
    "# Fill in 'county_fips' column with values from 'CountyFIPS', then convert to integer\n",
    "data['county_fips'] = data['CountyFIPS'].fillna(data['county_fips']).astype('Int64')\n",
    "data.drop(columns=['CountyFIPS'], inplace=True)\n",
    "\n",
    "# Save the modified data to a new CSV file\n",
    "output_path = './Input_Data/modified_unemployment_per_county.csv'\n",
    "data.to_csv(output_path, index=False)\n",
    "\n",
    "#Need to manually add FIPS code for Jefferson(21111), Miami-Dade(12025), Prince George's(). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e796fd",
   "metadata": {},
   "source": [
    "preprocess median income data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aeb9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the main data and county data for clarity and re-run merge process with updated checks\n",
    "# Load the original rotated data and county file again\n",
    "\n",
    "# Load the main data\n",
    "file_path = './Input_Data/median_income_by_county.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "rotated_data = data.T\n",
    "rotated_data.reset_index(inplace=True)\n",
    "rotated_data.columns = ['Attribute', '2017-01-01', '2018-01-01', '2019-01-01', '2020-01-01', '2021-01-01', '2022-01-01']\n",
    "\n",
    "# Extract \"County\" and \"State\" from Attribute\n",
    "rotated_data[['County', 'State']] = rotated_data['Attribute'].str.extract(r'for (.*), (\\w{2})')\n",
    "rotated_data.rename(columns={'County': 'RegionName'}, inplace=True)\n",
    "rotated_data.insert(2, 'RegionType', 'county')\n",
    "rotated_data.insert(3, 'StateName', rotated_data['State'])\n",
    "rotated_data['CountyName'] = rotated_data['RegionName'].replace(\n",
    "    {' County': '', 'Miami-': '', 'Louisville-': '', '/City': '', '/city': '', \"Prince George's\": \"Prince George\"}, regex=True\n",
    ")\n",
    "reordered_data = rotated_data[['Attribute', 'RegionName', 'RegionType', 'CountyName', 'StateName', 'State', '2017-01-01', '2018-01-01', \n",
    "                               '2019-01-01', '2020-01-01', '2021-01-01', '2022-01-01']]\n",
    "\n",
    "# Load county name file\n",
    "county_file_path = './Input_Data/countyname.csv'\n",
    "county_data = pd.read_csv(county_file_path)\n",
    "\n",
    "# Merge reordered_data with county_data based on StateName and CountyName\n",
    "merged_data = reordered_data.merge(\n",
    "    county_data[['StateCodeFIPS', 'CountyFIPS', 'CountyName', 'StateName']],\n",
    "    how='left',\n",
    "    on=['StateName', 'CountyName']\n",
    ")\n",
    "\n",
    "# Assign merged FIPS values to new columns and remove unnecessary columns if exists\n",
    "merged_data['StateCodeFIPS'] = merged_data['StateCodeFIPS']\n",
    "merged_data['county_fips'] = merged_data['CountyFIPS']\n",
    "merged_data = merged_data.drop(columns=['CountyFIPS'], errors='ignore')\n",
    "\n",
    "# Convert FIPS columns to integer type and reorder columns\n",
    "merged_data[\"StateCodeFIPS\"] = merged_data[\"StateCodeFIPS\"].astype(\"Int64\")\n",
    "merged_data[\"county_fips\"] = merged_data[\"county_fips\"].astype(\"Int64\")\n",
    "columns_order = ['Attribute', 'RegionName', 'RegionType', 'CountyName', 'StateName', 'State', \n",
    "                 'StateCodeFIPS', 'county_fips', '2017-01-01', '2018-01-01', '2019-01-01', \n",
    "                 '2020-01-01', '2021-01-01', '2022-01-01']\n",
    "final_data = merged_data[columns_order]\n",
    "\n",
    "# Save final data\n",
    "output_file_path = './Input_Data/modified_median_income_per_county.csv'\n",
    "final_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "output_file_path\n",
    "#remove the 1st row manually, add missing 2022 data with 2021 data for some rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182c2a81",
   "metadata": {},
   "source": [
    "preprocess homeownership per county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5a159c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the main data and county data for clarity and re-run merge process with updated checks\n",
    "# Load the original rotated data and county file again\n",
    "\n",
    "# Load the main data\n",
    "file_path = './Input_Data/homeowner_by_county.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "rotated_data = data.T\n",
    "rotated_data.reset_index(inplace=True)\n",
    "rotated_data.columns = ['Attribute', '2017-01-01', '2018-01-01', '2019-01-01', '2020-01-01', '2021-01-01', '2022-01-01']\n",
    "\n",
    "# Extract \"County\" and \"State\" from Attribute\n",
    "rotated_data[['County', 'State']] = rotated_data['Attribute'].str.extract(r'for (.*), (\\w{2})')\n",
    "rotated_data.rename(columns={'County': 'RegionName'}, inplace=True)\n",
    "rotated_data.insert(2, 'RegionType', 'county')\n",
    "rotated_data.insert(3, 'StateName', rotated_data['State'])\n",
    "rotated_data['CountyName'] = rotated_data['RegionName'].replace(\n",
    "    {' County': '', 'Miami-': '', 'Louisville-': '', '/City': '', '/city': '', \"Prince George's\": \"Prince George\"}, regex=True\n",
    ")\n",
    "reordered_data = rotated_data[['Attribute', 'RegionName', 'RegionType', 'CountyName', 'StateName', 'State', '2017-01-01', '2018-01-01', \n",
    "                               '2019-01-01', '2020-01-01', '2021-01-01', '2022-01-01']]\n",
    "\n",
    "# Load county name file\n",
    "county_file_path = './Input_Data/countyname.csv'\n",
    "county_data = pd.read_csv(county_file_path)\n",
    "\n",
    "# Merge reordered_data with county_data based on StateName and CountyName\n",
    "merged_data = reordered_data.merge(\n",
    "    county_data[['StateCodeFIPS', 'CountyFIPS', 'CountyName', 'StateName']],\n",
    "    how='left',\n",
    "    on=['StateName', 'CountyName']\n",
    ")\n",
    "\n",
    "# Assign merged FIPS values to new columns and remove unnecessary columns if exists\n",
    "merged_data['StateCodeFIPS'] = merged_data['StateCodeFIPS']\n",
    "merged_data['county_fips'] = merged_data['CountyFIPS']\n",
    "merged_data = merged_data.drop(columns=['CountyFIPS'], errors='ignore')\n",
    "\n",
    "# Convert FIPS columns to integer type and reorder columns\n",
    "merged_data[\"StateCodeFIPS\"] = merged_data[\"StateCodeFIPS\"].astype(\"Int64\")\n",
    "merged_data[\"county_fips\"] = merged_data[\"county_fips\"].astype(\"Int64\")\n",
    "columns_order = ['Attribute', 'RegionName', 'RegionType', 'CountyName', 'StateName', 'State', \n",
    "                 'StateCodeFIPS', 'county_fips', '2017-01-01', '2018-01-01', '2019-01-01', \n",
    "                 '2020-01-01', '2021-01-01', '2022-01-01']\n",
    "final_data = merged_data[columns_order]\n",
    "\n",
    "# Save final data\n",
    "output_file_path = './Input_Data/modified_homeowner_per_county.csv'\n",
    "final_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "output_file_path\n",
    "#remove the 1st row manually, add missing 2022 data with 2021 data for some rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e261cf0f",
   "metadata": {},
   "source": [
    "preprocess income inequality per county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863b10b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the original CSV file\n",
    "file_path = './Input_Data/income_inequality_by_county.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Transpose the data to swap rows and columns\n",
    "rotated_data = data.set_index('Unnamed: 0').transpose()\n",
    "\n",
    "# Separate county and state names into two columns\n",
    "rotated_data.reset_index(inplace=True)\n",
    "rotated_data[['County', 'State']] = rotated_data['index'].str.extract(r'Income Inequality in (.+) County, (\\w{2})')\n",
    "\n",
    "# Keep the original first column\n",
    "rotated_data['Original County and State'] = 'Income Inequality in ' + rotated_data['County'] + ' County, ' + rotated_data['State']\n",
    "columns = ['Original County and State'] + [col for col in rotated_data.columns if col != 'Original County and State']\n",
    "rotated_data = rotated_data[columns]\n",
    "\n",
    "# Rename \"Original County and State\" to \"index\"\n",
    "rotated_data.rename(columns={\"Original County and State\": \"index\"}, inplace=True)\n",
    "\n",
    "# Reorder \"County\" and \"State\" next to \"index\"\n",
    "columns = [\"index\", \"County\", \"State\"] + [col for col in rotated_data.columns if col not in [\"index\", \"County\", \"State\"]]\n",
    "rotated_data = rotated_data[columns]\n",
    "\n",
    "# Remove \"Miami-\" prefix from \"County\" column\n",
    "rotated_data['County'] = rotated_data['County'].str.replace('Miami-', '', regex=False)\n",
    "\n",
    "# Rename \"County\" to \"CountyName\"\n",
    "rotated_data.rename(columns={\"County\": \"CountyName\"}, inplace=True)\n",
    "\n",
    "# Copy \"State\" to a new column \"StateName\"\n",
    "rotated_data['StateName'] = rotated_data['State']\n",
    "\n",
    "# Reorder \"StateName\" next to \"CountyName\"\n",
    "columns = [\"index\", \"CountyName\", \"StateName\", \"State\"] + [col for col in rotated_data.columns if col not in [\"index\", \"CountyName\", \"StateName\", \"State\"]]\n",
    "rotated_data = rotated_data[columns]\n",
    "\n",
    "# Add a new \"RegionName\" column next to \"index\" and populate it with values from \"CountyName\"\n",
    "rotated_data.insert(1, \"RegionName\", rotated_data['CountyName'] + \" County\")\n",
    "\n",
    "# Add a \"RegionType\" column next to \"RegionName\" and fill with \"county\"\n",
    "rotated_data.insert(2, \"RegionType\", \"county\")\n",
    "\n",
    "# Load FIPS codes from the uploaded \"countyname.csv\"\n",
    "countyname_path = './Input_Data/countyname.csv'\n",
    "countyname_data = pd.read_csv(countyname_path)\n",
    "\n",
    "# Merge FIPS codes from countyname_data into rotated_data\n",
    "rotated_data = rotated_data.merge(\n",
    "    countyname_data[['StateCodeFIPS', 'CountyFIPS', 'CountyName', 'StateName']],\n",
    "    how='left',\n",
    "    left_on=['CountyName', 'StateName'],\n",
    "    right_on=['CountyName', 'StateName']\n",
    ")\n",
    "\n",
    "# Populate FIPS code columns in the main dataset and rename for clarity\n",
    "rotated_data['StateCodeFIPS'] = rotated_data['StateCodeFIPS']\n",
    "rotated_data['county_fips'] = rotated_data['CountyFIPS']\n",
    "rotated_data.drop(columns=['CountyFIPS'], inplace=True)\n",
    "\n",
    "# Reorder the columns based on the specified order\n",
    "columns_order = [\n",
    "    \"index\", \"RegionName\", \"RegionType\", \"CountyName\", \"StateName\", \n",
    "    \"StateCodeFIPS\", \"county_fips\"\n",
    "] + [col for col in rotated_data.columns if col not in [\n",
    "    \"index\", \"RegionName\", \"RegionType\", \"CountyName\", \"StateName\", \n",
    "    \"StateCodeFIPS\", \"county_fips\"\n",
    "]]\n",
    "rotated_data = rotated_data[columns_order]\n",
    "\n",
    "# Save the reordered dataset to a CSV file\n",
    "output_path = './Input_Data/modified_income_inequality_per_county.csv'\n",
    "rotated_data.to_csv(output_path, index=False)\n",
    "\n",
    "# Provide the link for download\n",
    "output_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bff2d70",
   "metadata": {},
   "source": [
    "mean commuting time per county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a2b496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load the CSV file to inspect its contents\n",
    "file_path = './Input_Data/meancommute_by_county.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Transpose the dataframe to rotate rows and columns\n",
    "rotated_data = data.transpose()\n",
    "\n",
    "# Reset the header using the first row as column names, and drop the first row\n",
    "rotated_data.columns = rotated_data.iloc[0]\n",
    "rotated_data = rotated_data.drop(rotated_data.index[0])\n",
    "\n",
    "# Reset the index and rename the first column as 'County'\n",
    "rotated_data.reset_index(inplace=True)\n",
    "rotated_data = rotated_data.rename(columns={'index': 'County'})\n",
    "\n",
    "# Set the first column as the index and rename it to 'Index'\n",
    "rotated_data.set_index('County', inplace=True)\n",
    "rotated_data.index.name = 'index'\n",
    "\n",
    "# Split the index into \"County\" and \"State\" based on the last comma in each entry\n",
    "rotated_data.index = rotated_data.index.str.split(', ')\n",
    "rotated_data[['County', 'State']] = pd.DataFrame(rotated_data.index.tolist(), index=rotated_data.index)\n",
    "\n",
    "# Move the new columns to the front\n",
    "rotated_data.reset_index(drop=True, inplace=True)\n",
    "rotated_data = rotated_data[['County', 'State'] + [col for col in rotated_data.columns if col not in ['County', 'State']]]\n",
    "\n",
    "# Copy the \"County\" column to a new column named \"Index\"\n",
    "rotated_data['index'] = rotated_data['County']\n",
    "\n",
    "# Move the \"Index\" column to the first position\n",
    "rotated_data = rotated_data[['index'] + [col for col in rotated_data.columns if col != 'index']]\n",
    "\n",
    "# Remove the prefix from each entry in the \"County\" column\n",
    "rotated_data['County'] = rotated_data['County'].str.replace(\"Mean Commuting Time for Workers (5-year estimate) in \", \"\", regex=False)\n",
    "\n",
    "# Remove \"County\" and \"Miami-\" from each entry in the \"County\" column\n",
    "rotated_data['County'] = rotated_data['County'].str.replace(\" County\", \"\", regex=False)\n",
    "rotated_data['County'] = rotated_data['County'].str.replace(\"Miami-\", \"\", regex=False)\n",
    "\n",
    "# Rename \"Prince George's\" to \"Prince George\" in the \"County\" column\n",
    "rotated_data['County'] = rotated_data['County'].str.replace(\"Prince George's\", \"Prince George\", regex=False)\n",
    "\n",
    "# Rename \"County\" to \"CountyName\"\n",
    "rotated_data = rotated_data.rename(columns={'County': 'CountyName'})\n",
    "\n",
    "# Copy \"State\" to a new column named \"StateName\" and place it to the left of \"State\"\n",
    "rotated_data.insert(rotated_data.columns.get_loc('State'), 'StateName', rotated_data['State'])\n",
    "\n",
    "# Copy \"CountyName\" to a new column named \"RegionName\" and place it to the left of \"CountyName\"\n",
    "rotated_data.insert(rotated_data.columns.get_loc('CountyName'), 'RegionName', rotated_data['CountyName'])\n",
    "\n",
    "# Append \"County\" to each value in the \"RegionName\" column\n",
    "rotated_data['RegionName'] = rotated_data['RegionName'] + \" County\"\n",
    "\n",
    "# Add a new column \"RegionType\" right to \"RegionName\" and fill it with the value \"county\"\n",
    "rotated_data.insert(rotated_data.columns.get_loc('RegionName') + 1, 'RegionType', 'county')\n",
    "\n",
    "# Add two new columns \"StateCodeFIPS\" and \"county_fips\" and place them right next to the \"State\" column\n",
    "#rotated_data.insert(rotated_data.columns.get_loc('State') + 1, 'StateCodeFIPS', '')\n",
    "#rotated_data.insert(rotated_data.columns.get_loc('State') + 2, 'county_fips', '')\n",
    "\n",
    "# Load FIPS codes from the uploaded \"countyname.csv\"\n",
    "countyname_path = './Input_Data/countyname.csv'\n",
    "countyname_data = pd.read_csv(countyname_path)\n",
    "#print(countyname_data[['CountyName', 'StateName', 'StateCodeFIPS', 'CountyFIPS']].head())\n",
    "# Merge FIPS codes from countyname_data into rotated_data\n",
    "rotated_data = rotated_data.merge(\n",
    "    countyname_data[['StateCodeFIPS', 'CountyFIPS', 'CountyName', 'StateName']],\n",
    "    how='left',\n",
    "    left_on=['CountyName', 'StateName'],\n",
    "    right_on=['CountyName', 'StateName']\n",
    ")\n",
    "\n",
    "# Debugging output to check the columns after merge\n",
    "#print(rotated_data.columns)\n",
    "#print(rotated_data[['CountyName', 'StateName', 'StateCodeFIPS', 'CountyFIPS']].head())\n",
    "\n",
    "# Populate FIPS code columns in the main dataset and rename for clarity\n",
    "rotated_data['StateCodeFIPS'] = rotated_data['StateCodeFIPS']\n",
    "rotated_data['county_fips'] = rotated_data['CountyFIPS']\n",
    "rotated_data.drop(columns=['CountyFIPS'], inplace=True)\n",
    "# Debug output for merged columns\n",
    "#print(rotated_data[['StateCodeFIPS', 'county_fips']].head())\n",
    "\n",
    "\n",
    "# Reorder the columns based on the specified order\n",
    "columns_order = [\n",
    "    \"index\", \"RegionName\", \"RegionType\", \"CountyName\", \"StateName\", \n",
    "    \"StateCodeFIPS\", \"county_fips\"\n",
    "] + [col for col in rotated_data.columns if col not in [\n",
    "    \"index\", \"RegionName\", \"RegionType\", \"CountyName\", \"StateName\", \n",
    "    \"StateCodeFIPS\", \"county_fips\"\n",
    "]]\n",
    "rotated_data = rotated_data[columns_order]\n",
    "\n",
    "# Save the reordered dataset to a CSV file\n",
    "output_path = './Input_Data/modified_mean_commutetime_per_county.csv'\n",
    "rotated_data.to_csv(output_path, index=False)\n",
    "\n",
    "# Provide the link for download\n",
    "output_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9599ac",
   "metadata": {},
   "source": [
    "percent of population below poverty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fa2dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load the initial dataset\n",
    "file_path = './Input_Data/poppoverty_by_county.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Filter columns that start with \"Percent of Population Below the Poverty Level (5-year estimate)\"\n",
    "poverty_columns = [col for col in data.columns if col.startswith(\"Percent of Population Below the Poverty Level (5-year estimate)\")]\n",
    "\n",
    "# Create a new DataFrame with only the selected columns and the original first column\n",
    "first_column_name = data.columns[0]\n",
    "poverty_data_with_first_column = pd.concat([data[[first_column_name]], data[poverty_columns]], axis=1)\n",
    "\n",
    "# Transpose the DataFrame to switch rows and columns\n",
    "rotated_data = poverty_data_with_first_column.T\n",
    "rotated_data.reset_index(inplace=True)\n",
    "rotated_data.columns = rotated_data.iloc[0]  # Set the first row as the header\n",
    "rotated_data = rotated_data.drop(0)  # Drop the first row which is now the header\n",
    "\n",
    "# Rename \"Unnamed: 0\" to \"index\"\n",
    "rotated_data.rename(columns={\"Unnamed: 0\": \"index\"}, inplace=True)\n",
    "\n",
    "# Separate \"County\" and \"State\" information\n",
    "rotated_data[['County', 'State']] = rotated_data['index'].str.extract(r'^(.*?),\\s*(\\w{2})$')\n",
    "rotated_data = rotated_data[['index', 'County', 'State'] + [col for col in rotated_data.columns if col not in ['index', 'County', 'State']]]\n",
    "\n",
    "# Remove \"Percent of Population Below the Poverty Level (5-year estimate) in\" from \"County\" column\n",
    "rotated_data['County'] = rotated_data['County'].str.replace(\"Percent of Population Below the Poverty Level (5-year estimate) in \", \"\", regex=False)\n",
    "\n",
    "# Rename \"County\" to \"CountyName\" and copy to \"RegionName\"\n",
    "rotated_data.rename(columns={\"County\": \"CountyName\"}, inplace=True)\n",
    "rotated_data['RegionName'] = rotated_data['CountyName']\n",
    "\n",
    "# Add \"RegionType\" column and fill with \"county\"\n",
    "rotated_data['RegionType'] = \"county\"\n",
    "\n",
    "# Remove terms like \"County\", \"/City\", etc. from \"CountyName\"\n",
    "rotated_data['CountyName'] = (\n",
    "    rotated_data['CountyName']\n",
    "    .str.replace(\" County\", \"\", regex=False)\n",
    "    .str.replace(\"/city\", \"\", regex=False)\n",
    "    .str.replace(\"/City\", \"\", regex=False)\n",
    "    .str.replace(\"Miami-\", \"\", regex=False)\n",
    ")\n",
    "\n",
    "# Add placeholders for \"StateCodeFIPS\" and \"county_fips\" columns\n",
    "rotated_data['StateCodeFIPS'] = None\n",
    "rotated_data['county_fips'] = None\n",
    "\n",
    "# Copy \"State\" to \"StateName\"\n",
    "rotated_data['StateName'] = rotated_data['State']\n",
    "\n",
    "#print(rotated_data[['CountyName', 'StateName', 'StateCodeFIPS', 'county_fips']].head())\n",
    "\n",
    "# Load the countyname.csv file for FIPS codes\n",
    "countyname_data = pd.read_csv('./Input_Data/countyname.csv')\n",
    "\n",
    "# Merge FIPS codes from countyname_data into rotated_data\n",
    "rotated_data = rotated_data.merge(\n",
    "    countyname_data[['StateCodeFIPS', 'CountyFIPS', 'CountyName', 'StateName']],\n",
    "    how='left',\n",
    "    left_on=['CountyName', 'StateName'],\n",
    "    right_on=['CountyName', 'StateName']\n",
    ")\n",
    "\n",
    "# Debugging output to check the columns after merge\n",
    "#print(rotated_data.columns)\n",
    "#print(rotated_data[['CountyName', 'StateName', 'StateCodeFIPS_y', 'CountyFIPS']].head())\n",
    "\n",
    "\n",
    "# Rename \"CountyFIPS\" to \"county_fips\"\n",
    "rotated_data['StateCodeFIPS'] = rotated_data['StateCodeFIPS_y']\n",
    "rotated_data['county_fips'] = rotated_data['CountyFIPS']\n",
    "rotated_data.drop(columns=['CountyFIPS'], inplace=True)\n",
    "rotated_data.drop(columns=['StateCodeFIPS_y'], inplace=True)\n",
    "rotated_data.drop(columns=['StateCodeFIPS_x'], inplace=True)\n",
    "#rotated_data.rename(columns={'CountyFIPS': 'county_fips'}, inplace=True)\n",
    "#rotated_data.rename(columns={'StateCodeFIPS_y': 'StateCodeFIPS'}, inplace=True)\n",
    "#print(rotated_data[['CountyName', 'StateName', 'StateCodeFIPS', 'county_fips']].head())\n",
    "\n",
    "# Reorder columns and add an \"index\" column at the beginning with row numbers\n",
    "ordered_reordered_data_final\n",
    "\n",
    "\n",
    "# Save the final DataFrame to a CSV file\n",
    "output_path = './Input_Data/modified_population_under_poverty_per_county.csv'\n",
    "rotated_data.to_csv(output_path, index=False)\n",
    "\n",
    "output_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656e4ec0",
   "metadata": {},
   "source": [
    "resident_population per county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91a9821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load the initial file to inspect its structure\n",
    "file_path = './Input_Data/respop_by_county.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Transpose the data to switch rows and columns\n",
    "transposed_data = data.set_index(\"Unnamed: 0\").transpose().reset_index()\n",
    "\n",
    "# Copy the \"index\" column to a new column named \"County\"\n",
    "transposed_data[\"County\"] = transposed_data[\"index\"]\n",
    "\n",
    "# Separate county and state from the \"County\" column\n",
    "transposed_data[['County', 'State']] = transposed_data['County'].str.extract(r'^(.*), ([A-Z]{2})')\n",
    "\n",
    "# Reorder columns to make \"County\" and \"State\" next to \"index\"\n",
    "columns_order = ['index', 'County', 'State'] + [col for col in transposed_data.columns if col not in ['index', 'County', 'State']]\n",
    "reordered_data = transposed_data[columns_order]\n",
    "\n",
    "# Clean the \"County\" column by removing specific phrases\n",
    "transformed_data = reordered_data.copy()\n",
    "transformed_data[\"County\"] = transformed_data[\"County\"].replace(\n",
    "    {\n",
    "        \"Resident Population in \": \"\",\n",
    "        \" County\": \"\",\n",
    "        \"/city\": \"\",\n",
    "        \"/City\": \"\",\n",
    "        \"Miami-\": \"\"\n",
    "    },\n",
    "    regex=True\n",
    ")\n",
    "\n",
    "# Copy \"County\" to a new column named \"RegionName\"\n",
    "transformed_data[\"RegionName\"] = transformed_data[\"County\"]\n",
    "\n",
    "# Add new columns with placeholders\n",
    "transformed_data[\"RegionType\"] = \"county\"\n",
    "transformed_data[\"StateCodeFIPS\"] = \"\"\n",
    "transformed_data[\"StateName\"] = transformed_data[\"State\"]\n",
    "transformed_data[\"county_fips\"] = \"\"\n",
    "\n",
    "# Reorder columns as specified\n",
    "reordered_columns = [\n",
    "    'index', 'RegionName', 'RegionType', 'County', 'StateName', \n",
    "    'State', 'StateCodeFIPS', 'county_fips'\n",
    "] + [col for col in transformed_data.columns if col not in [\n",
    "    'index', 'RegionName', 'RegionType', 'County', 'StateName', \n",
    "    'State', 'StateCodeFIPS', 'county_fips'\n",
    "]]\n",
    "reordered_data_final = transformed_data[reordered_columns]\n",
    "\n",
    "# Rename \"County\" to \"CountyName\"\n",
    "reordered_data_final = reordered_data_final.rename(columns={\"County\": \"CountyName\"})\n",
    "\n",
    "# Append \"County\" to values in \"RegionName\" column\n",
    "reordered_data_final[\"RegionName\"] = reordered_data_final[\"RegionName\"] + \" County\"\n",
    "\n",
    "# Load the county name file to merge FIPS codes\n",
    "county_file_path = './Input_Data/countyname.csv'\n",
    "county_data = pd.read_csv(county_file_path)\n",
    "#print(reordered_data_final[['CountyName', 'StateName', 'StateCodeFIPS', 'county_fips']].head())\n",
    "# Strip extra whitespace for consistency\n",
    "reordered_data_final[\"StateName\"] = reordered_data_final[\"StateName\"].str.strip()\n",
    "reordered_data_final[\"CountyName\"] = reordered_data_final[\"CountyName\"].str.strip()\n",
    "county_data[\"StateName\"] = county_data[\"StateName\"].str.strip()\n",
    "county_data[\"CountyName\"] = county_data[\"CountyName\"].str.strip()\n",
    "\n",
    "# Perform the merge to fill in \"StateCodeFIPS\" and \"county_fips\"\n",
    "reordered_data_final = reordered_data_final.merge(\n",
    "    countyname_data[['StateCodeFIPS', 'CountyFIPS', 'CountyName', 'StateName']],\n",
    "    how='left',\n",
    "    left_on=['CountyName', 'StateName'],\n",
    "    right_on=['CountyName', 'StateName']\n",
    ")\n",
    "#print(reordered_data_final.columns)\n",
    "#print(reordered_data_final[['CountyName', 'StateName', 'StateCodeFIPS_y', 'county_fips']].head())\n",
    "# Combine the FIPS codes and remove extra columns\n",
    "reordered_data_final['StateCodeFIPS'] = reordered_data_final['StateCodeFIPS_y']\n",
    "reordered_data_final['county_fips'] = reordered_data_final['CountyFIPS']\n",
    "reordered_data_final = reordered_data_final.drop(columns=['StateCodeFIPS_x', 'StateCodeFIPS_y', 'CountyFIPS'])\n",
    "#print(reordered_data_final[['CountyName', 'StateName', 'StateCodeFIPS', 'county_fips']].head())\n",
    "# Reorder the columns based on the specified order\n",
    "columns_order = [\n",
    "    \"index\", \"RegionName\", \"RegionType\", \"CountyName\", \"StateName\", \n",
    "    \"StateCodeFIPS\", \"county_fips\"\n",
    "] + [col for col in reordered_data_final.columns if col not in [\n",
    "    \"index\", \"RegionName\", \"RegionType\", \"CountyName\", \"StateName\", \n",
    "    \"StateCodeFIPS\", \"county_fips\"\n",
    "]]\n",
    "reordered_data_final = reordered_data_final[columns_order]\n",
    "# Save the final data to a CSV file\n",
    "output_file_path = './Input_Data/modified_resident_population_per_county.csv'\n",
    "reordered_data_final.to_csv(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdca3fb0",
   "metadata": {},
   "source": [
    "preprocess gdp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91522e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the provided CSV files\n",
    "gdp_by_county = pd.read_csv('./Input_Data/gdp_by_county.csv')\n",
    "county_names = pd.read_csv('./Input_Data/countyname.csv')\n",
    "\n",
    "# Transpose the GDP data so that each county becomes a row and each date a column\n",
    "gdp_by_county_rotated = gdp_by_county.set_index(gdp_by_county.columns[0]).transpose()\n",
    "gdp_by_county_rotated.reset_index(inplace=True)\n",
    "\n",
    "# Copy the \"index\" column to a new column named \"RegionName\"\n",
    "gdp_by_county_rotated['RegionName'] = gdp_by_county_rotated['index']\n",
    "\n",
    "# Reorder the columns to place \"RegionName\" next to \"index\"\n",
    "columns_order = ['index', 'RegionName'] + [col for col in gdp_by_county_rotated.columns if col not in ['index', 'RegionName']]\n",
    "gdp_by_county_rotated = gdp_by_county_rotated[columns_order]\n",
    "\n",
    "# Remove \"Gross Domestic Product: All Industries in\" from the values in the \"RegionName\" column\n",
    "gdp_by_county_rotated['RegionName'] = gdp_by_county_rotated['RegionName'].str.replace(\n",
    "    'Gross Domestic Product: All Industries in ', '', regex=False\n",
    ")\n",
    "\n",
    "# Separate \"RegionName\" into \"County\" and \"State\" columns\n",
    "gdp_by_county_rotated[['County', 'State']] = gdp_by_county_rotated['RegionName'].str.rsplit(', ', n=1, expand=True)\n",
    "\n",
    "# Reorder the columns to place \"County\" and \"State\" next to \"RegionName\"\n",
    "columns_order = ['index', 'RegionName', 'County', 'State'] + [col for col in gdp_by_county_rotated.columns if col not in ['index', 'RegionName', 'County', 'State']]\n",
    "gdp_by_county_rotated = gdp_by_county_rotated[columns_order]\n",
    "\n",
    "# Remove \"County\", \"/City\", \"/city\", \"Miami-\" from \"County\" column\n",
    "gdp_by_county_rotated['County'] = gdp_by_county_rotated['County'].str.replace(' County', '', regex=False)\n",
    "gdp_by_county_rotated['County'] = gdp_by_county_rotated['County'].str.replace('/City', '', regex=False)\n",
    "gdp_by_county_rotated['County'] = gdp_by_county_rotated['County'].str.replace('/city', '', regex=False)\n",
    "gdp_by_county_rotated['County'] = gdp_by_county_rotated['County'].str.replace('Miami-', '', regex=False)\n",
    "\n",
    "# Rename \"County\" column to \"CountyName\"\n",
    "gdp_by_county_rotated.rename(columns={'County': 'CountyName'}, inplace=True)\n",
    "\n",
    "# Add new columns with placeholder values\n",
    "gdp_by_county_rotated['RegionType'] = 'county'\n",
    "gdp_by_county_rotated['StateName'] = gdp_by_county_rotated['State']\n",
    "gdp_by_county_rotated['StateCodeFIPS'] = None\n",
    "gdp_by_county_rotated['county_fips'] = None\n",
    "\n",
    "# Merge with county_names to fill in StateCodeFIPS and county_fips values\n",
    "gdp_by_county_merged = gdp_by_county_rotated.merge(\n",
    "    county_names[['StateCodeFIPS', 'CountyFIPS', 'CountyName', 'StateName']],\n",
    "    how='left',\n",
    "    left_on=['CountyName', 'StateName'],\n",
    "    right_on=['CountyName', 'StateName']\n",
    ")\n",
    "#print(gdp_by_county_merged.columns)\n",
    "#print(gdp_by_county_merged[['CountyName', 'StateName', 'StateCodeFIPS_y', 'county_fips']].head())\n",
    "# Assign correct columns and clean up\n",
    "gdp_by_county_merged['county_fips'] = gdp_by_county_merged['CountyFIPS']\n",
    "gdp_by_county_merged['StateCodeFIPS'] = gdp_by_county_merged['StateCodeFIPS_y']\n",
    "gdp_by_county_merged = gdp_by_county_merged.drop(columns=['StateCodeFIPS_x', 'StateCodeFIPS_y', 'CountyFIPS'])\n",
    "#print(gdp_by_county_merged[['CountyName', 'StateName', 'StateCodeFIPS', 'county_fips']].head())\n",
    "columns_order = [\n",
    "    \"index\", \"RegionName\", \"RegionType\", \"CountyName\", \"StateName\", \n",
    "    \"StateCodeFIPS\", \"county_fips\"\n",
    "] + [col for col in gdp_by_county_merged.columns if col not in [\n",
    "    \"index\", \"RegionName\", \"RegionType\", \"CountyName\", \"StateName\", \n",
    "    \"StateCodeFIPS\", \"county_fips\"\n",
    "]]\n",
    "gdp_by_county_merged = gdp_by_county_merged[columns_order]\n",
    "# Save the modified DataFrame to a CSV file\n",
    "gdp_by_county_merged.to_csv('./Input_Data/modified_gdp_per_county.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb0f57c",
   "metadata": {},
   "source": [
    "preprocess total list per county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955f809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load the initial CSV file and county name file\n",
    "file_path = './Input_Data/totallist_by_county.csv'\n",
    "countyname_path = './Input_Data/countyname.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "county_data = pd.read_csv(countyname_path)\n",
    "\n",
    "# Transpose the data to switch columns and rows\n",
    "transposed_data = data.transpose()\n",
    "transposed_data.columns = transposed_data.iloc[0]  # Set the first row as header\n",
    "transposed_data = transposed_data.drop(transposed_data.index[0])  # Drop the first row as it's now the header\n",
    "transposed_data_with_index = transposed_data.reset_index()  # Reset index\n",
    "transposed_data_with_index.columns.values[0] = 'County/Inventory Metric'  # Rename index column\n",
    "transposed_data_with_index.rename(columns={'County/Inventory Metric': 'index'}, inplace=True)\n",
    "\n",
    "# Copy \"index\" column to create \"RegionName\"\n",
    "transposed_data_with_index['RegionName'] = transposed_data_with_index['index']\n",
    "\n",
    "# Simplify the \"RegionName\" column by removing \"Housing Inventory: Total Listing Count in\"\n",
    "transposed_data_with_index['RegionName'] = transposed_data_with_index['RegionName'].str.replace(\n",
    "    \"Housing Inventory: Total Listing Count in \", \"\", regex=False\n",
    ")\n",
    "\n",
    "# Split \"RegionName\" into \"County\" and \"State\" columns\n",
    "transposed_data_with_index[['County', 'State']] = transposed_data_with_index['RegionName'].str.rsplit(', ', n=1, expand=True)\n",
    "\n",
    "# Reorder columns to place \"County\" and \"State\" next to \"RegionName\"\n",
    "columns = list(transposed_data_with_index.columns)\n",
    "columns.insert(columns.index(\"RegionName\") + 1, columns.pop(columns.index(\"County\")))\n",
    "columns.insert(columns.index(\"RegionName\") + 2, columns.pop(columns.index(\"State\")))\n",
    "transposed_data_reordered = transposed_data_with_index[columns]\n",
    "\n",
    "# Remove specific terms from \"County\" column\n",
    "transposed_data_reordered['County'] = transposed_data_reordered['County'].replace(\n",
    "    [\" County\", \"/City\", \"/city\", \"Miami-\"], \"\", regex=True\n",
    ")\n",
    "\n",
    "# Add new columns \"RegionType\", \"StateName\", \"StateCodeFIPS\", and \"county_fips\"\n",
    "transposed_data_reordered['RegionType'] = None\n",
    "transposed_data_reordered['StateName'] = None\n",
    "transposed_data_reordered['StateCodeFIPS'] = None\n",
    "transposed_data_reordered['county_fips'] = None\n",
    "\n",
    "# Reorder the main columns as specified\n",
    "ordered_columns = [\n",
    "    \"index\", \"RegionName\", \"RegionType\", \"County\", \"StateName\", \"State\", \n",
    "    \"StateCodeFIPS\", \"county_fips\"\n",
    "] + [col for col in transposed_data_reordered.columns if col not in [\n",
    "    \"index\", \"RegionName\", \"RegionType\", \"County\", \"StateName\", \"State\", \n",
    "    \"StateCodeFIPS\", \"county_fips\"\n",
    "]]\n",
    "transposed_data_ordered = transposed_data_reordered[ordered_columns]\n",
    "\n",
    "# Fill \"RegionType\" with \"county\" and copy \"State\" values to \"StateName\"\n",
    "transposed_data_ordered['RegionType'] = 'county'\n",
    "transposed_data_ordered['StateName'] = transposed_data_ordered['State']\n",
    "\n",
    "# Rename \"County\" to \"CountyName\" for final presentation\n",
    "transposed_data_ordered.rename(columns={'County': 'CountyName'}, inplace=True)\n",
    "\n",
    "# Merge with county FIPS data to fill \"StateCodeFIPS\" and \"county_fips\"\n",
    "merged_data = transposed_data_ordered.merge(\n",
    "    county_data[['StateCodeFIPS', 'CountyFIPS', 'CountyName', 'StateName']],\n",
    "    how='left',\n",
    "    left_on=['StateName', 'CountyName'],\n",
    "    right_on=['StateName', 'CountyName']\n",
    ")\n",
    "\n",
    "#print(merged_data.columns)\n",
    "#print(merged_data[['CountyName', 'StateName', 'StateCodeFIPS_y', 'CountyFIPS']].head())\n",
    "# Assign correct columns and clean up\n",
    "merged_data['county_fips'] = merged_data['CountyFIPS']\n",
    "merged_data['StateCodeFIPS'] = merged_data['StateCodeFIPS_y']\n",
    "merged_data = merged_data.drop(columns=['StateCodeFIPS_x', 'StateCodeFIPS_y', 'CountyFIPS'])\n",
    "#print(gdp_by_county_merged[['CountyName', 'StateName', 'StateCodeFIPS', 'county_fips']].head())\n",
    "columns_order = [\n",
    "    \"index\", \"RegionName\", \"RegionType\", \"CountyName\", \"StateName\", \n",
    "    \"StateCodeFIPS\", \"county_fips\"\n",
    "] + [col for col in merged_data.columns if col not in [\n",
    "    \"index\", \"RegionName\", \"RegionType\", \"CountyName\", \"StateName\", \n",
    "    \"StateCodeFIPS\", \"county_fips\"\n",
    "]]\n",
    "merged_data = merged_data[columns_order]\n",
    "# Save the modified DataFrame to a CSV file\n",
    "merged_data.to_csv('./Input_Data/modified_totallisting_per_county.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318b3cbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dva-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
